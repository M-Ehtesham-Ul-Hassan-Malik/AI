{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.493783Z","iopub.execute_input":"2025-06-19T07:17:30.494477Z","iopub.status.idle":"2025-06-19T07:17:30.500259Z","shell.execute_reply.started":"2025-06-19T07:17:30.494442Z","shell.execute_reply":"2025-06-19T07:17:30.499044Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gym\nimport random\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.507043Z","iopub.execute_input":"2025-06-19T07:17:30.507423Z","iopub.status.idle":"2025-06-19T07:17:30.527624Z","shell.execute_reply.started":"2025-06-19T07:17:30.507399Z","shell.execute_reply":"2025-06-19T07:17:30.526572Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.529461Z","iopub.execute_input":"2025-06-19T07:17:30.529745Z","iopub.status.idle":"2025-06-19T07:17:30.547896Z","shell.execute_reply.started":"2025-06-19T07:17:30.529725Z","shell.execute_reply":"2025-06-19T07:17:30.546944Z"}},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":"# Part 1 Frozen Lake\n\nFrozen Lake is a classic environment in Reinforcement Learning (RL) used to demonstrate fundamental concepts such as value iteration, policy iteration, and Q-learning.\n\nFrozen Lake is an environment provided by OpenAI Gym (now Gymnasium) where an agent must learn to navigate a frozen lake to reach a goal without falling into holes.\n\n### Environment Description\nThe environment is typically a 4x4 or 8x8 grid:\n\nS – Start (initial state)\n\nF – Frozen (safe)\n\nH – Hole (terminal state, game over)\n\nG – Goal (terminal state, success)\n\n### RL Problem Setup\nStates (S): Grid cells (e.g., 16 states in a 4x4 map)\n\nActions (A): Left, Right, Up, Down\n\nReward:\n\n+1 for reaching the Goal (G)\n\n0 for every other state\n\nTransition Probabilities: The environment is stochastic. The agent might slip and not move in the intended direction.\n\n\n### Types of Frozen Lake\nis_slippery=True: Default; stochastic transitions\n\nis_slippery=False: Deterministic transitions (easier)\n\n\n### Goal\nThe agent must learn an optimal policy (π*) to maximize reward by safely reaching the goal, avoiding holes, and handling uncertainty due to slippage.\n\n### Solving Frozen Lake\nYou can apply:\n\n- Dynamic Programming\n    - Policy Iteration\n    - Value Iteration\n\n- Model-Free Methods\n    - Q-learning\n    - SARSA\n\n","metadata":{}},{"cell_type":"markdown","source":"# 4x4 Map and Non Sliperty","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Create and understand FrozenLake environment \n","metadata":{}},{"cell_type":"code","source":"# create the frozen lake environment using 4x4 Map and Non Sliperty version\nenv = gym.make(\"FrozenLake-v1\", map_name='4x4', is_slippery=False) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.549336Z","iopub.execute_input":"2025-06-19T07:17:30.549669Z","iopub.status.idle":"2025-06-19T07:17:30.566356Z","shell.execute_reply.started":"2025-06-19T07:17:30.549639Z","shell.execute_reply":"2025-06-19T07:17:30.565312Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"env.reset() # This resets the environment to its initial state.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.567803Z","iopub.execute_input":"2025-06-19T07:17:30.568131Z","iopub.status.idle":"2025-06-19T07:17:30.584224Z","shell.execute_reply.started":"2025-06-19T07:17:30.568102Z","shell.execute_reply":"2025-06-19T07:17:30.583295Z"}},"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":103},{"cell_type":"markdown","source":"**Agent has 4 discrete actions**\n\n| Action Index | Action Name |\n| ------------ | ----------- |\n| 0            | Left        |\n| 1            | Down        |\n| 2            | Right       |\n| 3            | Up          |\n","metadata":{}},{"cell_type":"code","source":"print('\\n ____Action Space____ \\n')\nprint('Action Space Shape: ', env.action_space.n)\nprint('Action Space Sample: ', env.action_space.sample())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.596049Z","iopub.execute_input":"2025-06-19T07:17:30.596400Z","iopub.status.idle":"2025-06-19T07:17:30.603088Z","shell.execute_reply.started":"2025-06-19T07:17:30.596367Z","shell.execute_reply":"2025-06-19T07:17:30.602268Z"}},"outputs":[{"name":"stdout","text":"\n ____Action Space____ \n\nAction Space Shape:  4\nAction Space Sample:  3\n","output_type":"stream"}],"execution_count":104},{"cell_type":"markdown","source":"### Frozen Lake Rewards\n**In the FrozenLake-v1 environment**:\n\n| State Type    | Reward | Description                       |\n| ------------- | ------ | --------------------------------- |\n| Safe Tile (F) | 0      | No reward for just moving         |\n| Hole (H)      | 0      | Episode ends, no reward (failure) |\n| Goal (G)      | 1      | Success! You reached the goal     |\n","metadata":{}},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(f'There are {state_space} possible states. \\n')\naction_space = env.action_space.n\nprint(f'There are {action_space} possible actions. \\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.615705Z","iopub.execute_input":"2025-06-19T07:17:30.616401Z","iopub.status.idle":"2025-06-19T07:17:30.620942Z","shell.execute_reply.started":"2025-06-19T07:17:30.616375Z","shell.execute_reply":"2025-06-19T07:17:30.619986Z"}},"outputs":[{"name":"stdout","text":"There are 16 possible states. \n\nThere are 4 possible actions. \n\n","output_type":"stream"}],"execution_count":105},{"cell_type":"markdown","source":"## Step 2: Create and Initialize the Q-table","metadata":{}},{"cell_type":"code","source":"# lets create QTable of size(state_space, action_space) & Initialize each value at 0 using np.zeros\n\ndef initialize_q_table(state_space, action_space):\n    qTable = np.zeros((state_space, action_space))\n\n    return qTable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.628753Z","iopub.execute_input":"2025-06-19T07:17:30.629021Z","iopub.status.idle":"2025-06-19T07:17:30.639500Z","shell.execute_reply.started":"2025-06-19T07:17:30.628994Z","shell.execute_reply":"2025-06-19T07:17:30.638402Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"qTable_frozenLake = initialize_q_table(state_space, action_space)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.643547Z","iopub.execute_input":"2025-06-19T07:17:30.643812Z","iopub.status.idle":"2025-06-19T07:17:30.655415Z","shell.execute_reply.started":"2025-06-19T07:17:30.643791Z","shell.execute_reply":"2025-06-19T07:17:30.654352Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"qTable_frozenLake","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.668225Z","iopub.execute_input":"2025-06-19T07:17:30.669035Z","iopub.status.idle":"2025-06-19T07:17:30.676048Z","shell.execute_reply.started":"2025-06-19T07:17:30.669008Z","shell.execute_reply":"2025-06-19T07:17:30.675105Z"}},"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"},"metadata":{}}],"execution_count":108},{"cell_type":"markdown","source":"## step 3: Define Epsilon and Greedy Policy\n\nThe epsilon-greedy policy helps the agent balance between:\n\n- Exploration (try new actions)\n- Exploitation (use the best-known action)\n\n**Parameters**:\n| Parameter      | Description                                   |\n| -------------- | --------------------------------------------- |\n| `q_table`      | The Q-table (state × action values)           |\n| `state`        | Current state of the agent                    |\n| `epsilon`      | Exploration rate (e.g., 0.1 means 10% random) |\n| `action_space` | Number of possible actions                    |\n","metadata":{}},{"cell_type":"markdown","source":"### Epsilon Greedy Policy (Acting Policy)","metadata":{}},{"cell_type":"code","source":"def epsilon_greedy_policy(qTable, state, epsilon):\n\n    random_num = random.uniform(0,1) # Randomly generate number b/w 0 & 1.\n    if random_num > epsilon:         #    if random_num > epsilon: ---> exploitation\n        # Take the action with the heighest value\n        # Agent will choose the action with highest q-value\n        action = np.argmax(qTable[state])\n\n    else: #        else: -----> exploration \n\n        action = env.action_space.sample() # Take a random action\n\n\n    return action     \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.680593Z","iopub.execute_input":"2025-06-19T07:17:30.681364Z","iopub.status.idle":"2025-06-19T07:17:30.691853Z","shell.execute_reply.started":"2025-06-19T07:17:30.681324Z","shell.execute_reply":"2025-06-19T07:17:30.690944Z"}},"outputs":[],"execution_count":109},{"cell_type":"markdown","source":"### On-Policy vs Off-Policy in Reinforcement Learning\nThese terms refer to how the agent learns from experience. Specifically, whether it learns from the same policy it uses to act or from a different one.\n\n- Off-Policy: using a diffrent policy for acting and for updating\n    - Q-Learning \n- On-Policy: using the same policy for acting and for updating\n    - Example: SARSA\n\n\nRemember:\n- Epsilon Greedy Policy (Acting Policy)\n- Greedy Policy (Updating Policy)\n","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Define the greedy policy ","metadata":{}},{"cell_type":"markdown","source":"### Greedy Policy (Updating Policy)","metadata":{}},{"cell_type":"code","source":"def greedy_policy(qTable, state):\n    # Exploitation: Take the action with the heighest state & action-value\n    action = np.argmax(qTable[state])\n\n    return action","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.695398Z","iopub.execute_input":"2025-06-19T07:17:30.696267Z","iopub.status.idle":"2025-06-19T07:17:30.711648Z","shell.execute_reply.started":"2025-06-19T07:17:30.696223Z","shell.execute_reply":"2025-06-19T07:17:30.710561Z"}},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":"## Step 5: Define Hyperparameters ","metadata":{}},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 10000  # Total training episodes\nlearning_rate = 0.7          # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"     # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate # This is the discounting rate, which determines the importance of future rewards. A value close to 1 means future rewards are highly considered, while a value close to 0 means the agent prioritizes immediate rewards.\neval_seed = []               # The evaluation seed of the environment\n\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05            # Minimum exploration probability\ndecay_rate = 0.0005            # Exponential decay rate for exploration prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.713005Z","iopub.execute_input":"2025-06-19T07:17:30.713477Z","iopub.status.idle":"2025-06-19T07:17:30.728437Z","shell.execute_reply.started":"2025-06-19T07:17:30.713447Z","shell.execute_reply":"2025-06-19T07:17:30.727286Z"}},"outputs":[],"execution_count":111},{"cell_type":"markdown","source":"## Step 6: Create Trainig Loop Method","metadata":{}},{"cell_type":"code","source":"def train (n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, qTable):\n    for episode in range(n_training_episodes):\n\n        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) # Reduce epsilon (because we need less and less exploration)\n\n        # Reset Environment\n        state = env.reset()\n        step = 0\n        done = False\n\n        # Repeat\n        for step in range(max_steps):\n\n            # choose action using epsilon greedy policy\n            action = epsilon_greedy_policy(qTable, state, epsilon)\n\n            # take action and observe Rt+1 and St+1\n            # take action (A), observe the outcome state (S), and reward (R)\n            new_state, reward, done, info = env.step(action) # take choosen action to the environment \n\n\n            # update the Q-value for the state-action pair using the Q-Learning Formula\n            qTable[state][action] = qTable[state][action] +  learning_rate * (reward + gamma * np.max(qTable[new_state]) - qTable[state][action])\n\n            # if done, finish the episode\n            if done:\n                break\n\n\n            # now our state is the new state\n            state = new_state\n\n\n    return qTable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.730277Z","iopub.execute_input":"2025-06-19T07:17:30.731110Z","iopub.status.idle":"2025-06-19T07:17:30.747789Z","shell.execute_reply.started":"2025-06-19T07:17:30.731075Z","shell.execute_reply":"2025-06-19T07:17:30.746776Z"}},"outputs":[],"execution_count":112},{"cell_type":"markdown","source":"## Step 7: Train the The Q-Learning Agent","metadata":{}},{"cell_type":"code","source":"q_Table_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, qTable_frozenLake)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:30.749151Z","iopub.execute_input":"2025-06-19T07:17:30.749568Z","iopub.status.idle":"2025-06-19T07:17:32.574043Z","shell.execute_reply.started":"2025-06-19T07:17:30.749538Z","shell.execute_reply":"2025-06-19T07:17:32.572983Z"}},"outputs":[],"execution_count":113},{"cell_type":"markdown","source":"## Step 8: Let's see the Q-Learning Table","metadata":{}},{"cell_type":"code","source":"qTable_frozenLake","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:17:32.575531Z","iopub.execute_input":"2025-06-19T07:17:32.575848Z","iopub.status.idle":"2025-06-19T07:17:32.583442Z","shell.execute_reply.started":"2025-06-19T07:17:32.575821Z","shell.execute_reply":"2025-06-19T07:17:32.582515Z"}},"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n       [0.73509189, 0.        , 0.81450625, 0.77378094],\n       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n       [0.81450625, 0.        , 0.77378094, 0.77378094],\n       [0.77378094, 0.81450625, 0.        , 0.73509189],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.        , 0.81450625],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.81450625, 0.        , 0.857375  , 0.77378094],\n       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n       [0.857375  , 0.95      , 0.        , 0.857375  ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.95      , 0.857375  ],\n       [0.9025    , 0.95      , 1.        , 0.9025    ],\n       [0.        , 0.        , 0.        , 0.        ]])"},"metadata":{}}],"execution_count":114},{"cell_type":"markdown","source":"## Step 9: Define the Evaluation Method","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 10: Evaluate our Q-Learning Agent","metadata":{}},{"cell_type":"code","source":"def evaluate_agent(env, qTable, n_eval_episodes, max_steps):\n    rewards_per_episode = []\n\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        done = False\n        episode_reward = 0\n\n        for step in range(max_steps):\n            action = np.argmax(qTable[state])  # Always exploit (greedy)\n            new_state, reward, done, info = env.step(action)\n            episode_reward += reward\n\n            if done:\n                break\n\n            state = new_state\n\n        rewards_per_episode.append(episode_reward)\n\n    # calculate avg reward\n    avg_reward = np.mean(rewards_per_episode)\n\n    # calculate std deviation of rewards\n    std_reward = np.std(rewards_per_episode)\n\n   \n\n    return avg_reward, std_reward\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:18:09.244035Z","iopub.execute_input":"2025-06-19T07:18:09.244469Z","iopub.status.idle":"2025-06-19T07:18:09.251725Z","shell.execute_reply.started":"2025-06-19T07:18:09.244440Z","shell.execute_reply":"2025-06-19T07:18:09.250510Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"# Evaluate the trained agent\nmean_reward, std_reward = evaluate_agent(env, q_Table_frozenlake, n_eval_episodes=100, max_steps=99) \nprint(f\"Mean Reward: {mean_reward} +/- {std_reward}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:22:03.301500Z","iopub.execute_input":"2025-06-19T07:22:03.301803Z","iopub.status.idle":"2025-06-19T07:22:03.336034Z","shell.execute_reply.started":"2025-06-19T07:22:03.301782Z","shell.execute_reply":"2025-06-19T07:22:03.334997Z"}},"outputs":[{"name":"stdout","text":"Mean Reward: 1.0 +/- 0.0\n","output_type":"stream"}],"execution_count":122},{"cell_type":"markdown","source":"# 8x8 Map and Non Sliperty","metadata":{}},{"cell_type":"code","source":"env = gym.make(\"FrozenLake-v1\", map_name='8x8', is_slippery=True) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:59:00.203442Z","iopub.execute_input":"2025-06-19T07:59:00.204231Z","iopub.status.idle":"2025-06-19T07:59:00.209872Z","shell.execute_reply.started":"2025-06-19T07:59:00.204196Z","shell.execute_reply":"2025-06-19T07:59:00.208882Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"env.reset() # This resets the environment to its initial state.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:59:00.727491Z","iopub.execute_input":"2025-06-19T07:59:00.728278Z","iopub.status.idle":"2025-06-19T07:59:00.733834Z","shell.execute_reply.started":"2025-06-19T07:59:00.728252Z","shell.execute_reply":"2025-06-19T07:59:00.733209Z"}},"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"print('\\n ____Action Space____ \\n')\nprint('Action Space Shape: ', env.action_space.n)\nprint('Action Space Sample: ', env.action_space.sample())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:59:01.279673Z","iopub.execute_input":"2025-06-19T07:59:01.279947Z","iopub.status.idle":"2025-06-19T07:59:01.285062Z","shell.execute_reply.started":"2025-06-19T07:59:01.279929Z","shell.execute_reply":"2025-06-19T07:59:01.284186Z"}},"outputs":[{"name":"stdout","text":"\n ____Action Space____ \n\nAction Space Shape:  4\nAction Space Sample:  2\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(f'There are {state_space} possible states. \\n')\naction_space = env.action_space.n\nprint(f'There are {action_space} possible actions. \\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:59:18.645868Z","iopub.execute_input":"2025-06-19T07:59:18.646379Z","iopub.status.idle":"2025-06-19T07:59:18.650964Z","shell.execute_reply.started":"2025-06-19T07:59:18.646353Z","shell.execute_reply":"2025-06-19T07:59:18.650107Z"}},"outputs":[{"name":"stdout","text":"There are 64 possible states. \n\nThere are 4 possible actions. \n\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"# lets create QTable of size(state_space, action_space) & Initialize each value at 0 using np.zeros\n\ndef initialize_q_table(state_space, action_space):\n    qTable = np.zeros((state_space, action_space))\n\n    return qTable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:59:27.743720Z","iopub.execute_input":"2025-06-19T07:59:27.744365Z","iopub.status.idle":"2025-06-19T07:59:27.748504Z","shell.execute_reply.started":"2025-06-19T07:59:27.744339Z","shell.execute_reply":"2025-06-19T07:59:27.747551Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"qTable_frozenLake = initialize_q_table(state_space, action_space)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:59:32.200374Z","iopub.execute_input":"2025-06-19T07:59:32.200973Z","iopub.status.idle":"2025-06-19T07:59:32.205017Z","shell.execute_reply.started":"2025-06-19T07:59:32.200948Z","shell.execute_reply":"2025-06-19T07:59:32.204118Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"qTable_frozenLake","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T07:59:36.623982Z","iopub.execute_input":"2025-06-19T07:59:36.624317Z","iopub.status.idle":"2025-06-19T07:59:36.631555Z","shell.execute_reply.started":"2025-06-19T07:59:36.624291Z","shell.execute_reply":"2025-06-19T07:59:36.630622Z"}},"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"},"metadata":{}}],"execution_count":135},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 100000  # Total training episodes\nlearning_rate = 0.7          # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"     # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate # This is the discounting rate, which determines the importance of future rewards. A value close to 1 means future rewards are highly considered, while a value close to 0 means the agent prioritizes immediate rewards.\neval_seed = []               # The evaluation seed of the environment\n\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05            # Minimum exploration probability\ndecay_rate = 0.0005            # Exponential decay rate for exploration prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:01:40.290738Z","iopub.execute_input":"2025-06-19T08:01:40.291043Z","iopub.status.idle":"2025-06-19T08:01:40.296591Z","shell.execute_reply.started":"2025-06-19T08:01:40.291020Z","shell.execute_reply":"2025-06-19T08:01:40.295738Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"q_Table_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, qTable_frozenLake)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:01:44.666790Z","iopub.execute_input":"2025-06-19T08:01:44.667739Z","iopub.status.idle":"2025-06-19T08:04:17.314299Z","shell.execute_reply.started":"2025-06-19T08:01:44.667708Z","shell.execute_reply":"2025-06-19T08:04:17.313502Z"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"qTable_frozenLake","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:04:17.315829Z","iopub.execute_input":"2025-06-19T08:04:17.316705Z","iopub.status.idle":"2025-06-19T08:04:17.325762Z","shell.execute_reply.started":"2025-06-19T08:04:17.316680Z","shell.execute_reply":"2025-06-19T08:04:17.324840Z"}},"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"array([[1.81363345e-02, 1.80563345e-02, 2.30345571e-02, 2.00733203e-02],\n       [1.94641801e-02, 2.99256040e-02, 2.17363368e-02, 2.01546397e-02],\n       [2.76213574e-02, 2.31521188e-02, 2.93919006e-02, 2.21842464e-02],\n       [3.20111985e-02, 3.06481745e-02, 3.18897461e-02, 3.79085643e-02],\n       [4.14165645e-02, 4.17103057e-02, 4.33028097e-02, 3.65310493e-02],\n       [4.42770779e-02, 4.71268699e-02, 5.27674247e-02, 4.83307021e-02],\n       [5.25656555e-02, 5.47252267e-02, 6.21033347e-02, 5.44383942e-02],\n       [5.60557451e-02, 5.34360518e-02, 5.98815844e-02, 5.43024625e-02],\n       [1.68839894e-02, 1.70022841e-02, 1.73344822e-02, 1.88616813e-02],\n       [1.73236423e-02, 1.58165986e-02, 2.10693816e-02, 1.77630947e-02],\n       [1.98732689e-02, 2.04758864e-02, 2.61888758e-02, 2.06865151e-02],\n       [1.30096149e-02, 5.52828272e-03, 8.76824808e-03, 3.68543437e-02],\n       [3.51152354e-02, 3.06177701e-02, 3.05471278e-02, 5.18237891e-02],\n       [4.89040633e-02, 5.04186428e-02, 5.17815002e-02, 4.22804284e-02],\n       [5.87271641e-02, 6.10697903e-02, 6.04441554e-02, 6.40240676e-02],\n       [6.39033560e-02, 1.01754361e-01, 6.35307569e-02, 6.27547169e-02],\n       [1.41152380e-02, 1.36925513e-02, 1.42004055e-02, 1.59153206e-02],\n       [1.49143583e-02, 7.59621604e-03, 8.55112504e-03, 1.63325864e-02],\n       [1.69810291e-02, 8.04453646e-04, 8.79147614e-04, 7.20215374e-04],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.30928658e-02, 5.97891747e-03, 1.10868325e-02, 4.38518030e-03],\n       [4.07354689e-02, 4.18092760e-02, 4.03115383e-02, 4.91545357e-02],\n       [5.66148334e-02, 5.97485172e-02, 6.46447267e-02, 5.29822763e-02],\n       [7.47197201e-02, 8.03127606e-02, 7.80320702e-02, 8.12055892e-02],\n       [7.89688404e-03, 4.91056463e-03, 1.08360405e-02, 1.37060112e-02],\n       [4.60197802e-03, 3.08158265e-03, 4.49715475e-03, 1.52267875e-02],\n       [9.32296535e-04, 5.54184720e-04, 1.33135049e-03, 4.77765150e-03],\n       [4.44921038e-05, 3.58163068e-04, 3.19680479e-04, 3.10312305e-04],\n       [1.37801679e-03, 2.98260917e-03, 5.38082971e-04, 1.14094238e-03],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.89487198e-02, 6.90575859e-03, 8.93428058e-02, 4.26116535e-03],\n       [1.75416795e-01, 1.35112524e-01, 1.03864776e-01, 1.00634678e-01],\n       [2.80643282e-03, 4.16528895e-03, 1.88784480e-03, 1.27416904e-02],\n       [1.90178932e-04, 4.78905309e-04, 1.51719388e-04, 1.39150944e-03],\n       [6.42127713e-05, 1.75978698e-05, 2.46414307e-05, 5.19182436e-05],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.25813381e-03, 1.23063771e-03, 3.83450131e-03, 2.76508402e-04],\n       [8.64302433e-04, 9.48367731e-03, 1.40863826e-02, 1.10271203e-02],\n       [2.02077753e-02, 5.09600749e-03, 1.55062192e-02, 9.48956266e-02],\n       [7.47718750e-02, 7.06746769e-02, 3.06699578e-01, 1.18586631e-01],\n       [1.73222784e-03, 5.67204612e-04, 7.84976940e-04, 5.92001693e-04],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.41009247e-07, 8.06675334e-05, 3.74512532e-06, 5.64721121e-05],\n       [4.85208158e-04, 8.69353928e-04, 3.13128849e-04, 4.59963372e-04],\n       [2.90394003e-02, 2.75210206e-03, 3.49255826e-03, 2.50873633e-03],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [6.14136944e-02, 1.83027610e-01, 6.77971586e-01, 7.50857702e-02],\n       [1.83481379e-03, 6.74732057e-04, 1.86113216e-04, 6.23718538e-04],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [4.26232687e-08, 9.19502882e-07, 2.80910158e-04, 1.61557483e-06],\n       [1.24318539e-06, 2.46590268e-05, 1.04929007e-06, 2.67085528e-05],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.21762356e-02, 1.03933783e-05, 1.87461500e-04, 1.25044506e-04],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.37311040e-01, 5.90399873e-02, 9.45374913e-01, 5.51817326e-01],\n       [1.17507840e-03, 9.02557254e-04, 1.11481024e-03, 1.07295255e-03],\n       [2.91775770e-04, 7.24177160e-04, 6.16655375e-05, 4.92838765e-06],\n       [6.37753229e-04, 5.31827278e-04, 1.27613888e-05, 3.68386468e-06],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.63136066e-03, 3.77739770e-01, 1.41481327e-02, 0.00000000e+00],\n       [2.77347655e-01, 1.70517885e-01, 6.19710474e-01, 2.18559175e-01],\n       [4.34788277e-01, 9.73286009e-01, 2.29350550e-01, 2.93879558e-01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"},"metadata":{}}],"execution_count":142},{"cell_type":"code","source":"# Evaluate the trained agent\nmean_reward, std_reward = evaluate_agent(env, q_Table_frozenlake, n_eval_episodes=100, max_steps=99) \nprint(f\"Mean Reward: {mean_reward} +/- {std_reward}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:09:04.754131Z","iopub.execute_input":"2025-06-19T08:09:04.754554Z","iopub.status.idle":"2025-06-19T08:09:04.873349Z","shell.execute_reply.started":"2025-06-19T08:09:04.754529Z","shell.execute_reply":"2025-06-19T08:09:04.872241Z"}},"outputs":[{"name":"stdout","text":"Mean Reward: 0.07 +/- 0.25514701644346144\n","output_type":"stream"}],"execution_count":144},{"cell_type":"markdown","source":"# Part 2: Taxi-v3 🚖\n\n### Step 1: Create and understand [Taxi-v3 🚕](https://www.gymlibrary.ml/environments/toy_text/taxi/)\n---\n\n💡 A good habit when you start to use an environment is to check its documentation\n\n👉 https://www.gymlibrary.ml/environments/toy_text/taxi/\n\n---\n\nIn Taxi-v3 🚕, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger’s location, picks up the passenger, drives to the passenger’s destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n\n\n| Action                          | Reward  |\n| ------------------------------- | ------- |\n| Each time step (move)           | **–1**  |\n| Illegal pickup/dropoff          | **–10** |\n| Successful dropoff (goal state) | **+20** |\n\n### What is a “Good” Average Reward?\n\n| Avg. Reward per Episode | Rating        | Comment                                              |\n| ----------------------- | ------------- | ---------------------------------------------------- |\n| **+8 to +20**           | ⭐ Excellent   | Fast and optimal routes with no illegal moves        |\n| **0 to +7**             | ✅ Acceptable  | A few wrong turns or minor delays                    |\n| **–10 to 0**            | ⚠️ Needs Work | Likely illegal pickups/dropoffs or inefficient moves |\n| **< –10**               | ❌ Poor        | Many penalties, aimless wandering                    |\n\n","metadata":{}},{"cell_type":"code","source":"env = gym.make(\"Taxi-v3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:14:24.161463Z","iopub.execute_input":"2025-06-19T08:14:24.161830Z","iopub.status.idle":"2025-06-19T08:14:24.250566Z","shell.execute_reply.started":"2025-06-19T08:14:24.161804Z","shell.execute_reply":"2025-06-19T08:14:24.249770Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"env.reset() # This resets the environment to its initial state.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:15:23.844307Z","iopub.execute_input":"2025-06-19T08:15:23.844957Z","iopub.status.idle":"2025-06-19T08:15:23.850694Z","shell.execute_reply.started":"2025-06-19T08:15:23.844934Z","shell.execute_reply":"2025-06-19T08:15:23.849736Z"}},"outputs":[{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"209"},"metadata":{}}],"execution_count":146},{"cell_type":"code","source":"print('\\n ____Action Space____ \\n')\nprint('Action Space Shape: ', env.action_space.n)\nprint('Action Space Sample: ', env.action_space.sample())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:15:32.150891Z","iopub.execute_input":"2025-06-19T08:15:32.151197Z","iopub.status.idle":"2025-06-19T08:15:32.156264Z","shell.execute_reply.started":"2025-06-19T08:15:32.151151Z","shell.execute_reply":"2025-06-19T08:15:32.155256Z"}},"outputs":[{"name":"stdout","text":"\n ____Action Space____ \n\nAction Space Shape:  6\nAction Space Sample:  4\n","output_type":"stream"}],"execution_count":147},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(f'There are {state_space} possible states. \\n')\naction_space = env.action_space.n\nprint(f'There are {action_space} possible actions. \\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:16:00.145119Z","iopub.execute_input":"2025-06-19T08:16:00.145467Z","iopub.status.idle":"2025-06-19T08:16:00.150703Z","shell.execute_reply.started":"2025-06-19T08:16:00.145444Z","shell.execute_reply":"2025-06-19T08:16:00.149707Z"}},"outputs":[{"name":"stdout","text":"There are 500 possible states. \n\nThere are 6 possible actions. \n\n","output_type":"stream"}],"execution_count":148},{"cell_type":"code","source":"# lets create QTable of size(state_space, action_space) & Initialize each value at 0 using np.zeros\n\ndef initialize_q_table(state_space, action_space):\n    qTable = np.zeros((state_space, action_space))\n\n    return qTable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:16:21.947765Z","iopub.execute_input":"2025-06-19T08:16:21.948042Z","iopub.status.idle":"2025-06-19T08:16:21.952895Z","shell.execute_reply.started":"2025-06-19T08:16:21.948023Z","shell.execute_reply":"2025-06-19T08:16:21.951724Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"qTable_taxi = initialize_q_table(state_space, action_space)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:16:40.324962Z","iopub.execute_input":"2025-06-19T08:16:40.325392Z","iopub.status.idle":"2025-06-19T08:16:40.329756Z","shell.execute_reply.started":"2025-06-19T08:16:40.325367Z","shell.execute_reply":"2025-06-19T08:16:40.328812Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"qTable_taxi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:16:46.902315Z","iopub.execute_input":"2025-06-19T08:16:46.903041Z","iopub.status.idle":"2025-06-19T08:16:46.909030Z","shell.execute_reply.started":"2025-06-19T08:16:46.903010Z","shell.execute_reply":"2025-06-19T08:16:46.908101Z"}},"outputs":[{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])"},"metadata":{}}],"execution_count":151},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 500000  # Total training episodes\nlearning_rate = 0.7          # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# Environment parameters\nenv_id = \"Taxi-v3\"           # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate # This is the discounting rate, which determines the importance of future rewards. A value close to 1 means future rewards are highly considered, while a value close to 0 means the agent prioritizes immediate rewards.\neval_seed = []               # The evaluation seed of the environment\n\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05            # Minimum exploration probability\ndecay_rate = 0.0005            # Exponential decay rate for exploration prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:18:36.637746Z","iopub.execute_input":"2025-06-19T08:18:36.638050Z","iopub.status.idle":"2025-06-19T08:18:36.643216Z","shell.execute_reply.started":"2025-06-19T08:18:36.638030Z","shell.execute_reply":"2025-06-19T08:18:36.642382Z"}},"outputs":[],"execution_count":157},{"cell_type":"code","source":"qTable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, qTable_taxi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:18:40.290339Z","iopub.execute_input":"2025-06-19T08:18:40.290632Z","iopub.status.idle":"2025-06-19T08:22:04.600120Z","shell.execute_reply.started":"2025-06-19T08:18:40.290613Z","shell.execute_reply":"2025-06-19T08:22:04.598989Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"qTable_taxi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:22:04.601715Z","iopub.execute_input":"2025-06-19T08:22:04.602043Z","iopub.status.idle":"2025-06-19T08:22:04.608569Z","shell.execute_reply.started":"2025-06-19T08:22:04.602013Z","shell.execute_reply":"2025-06-19T08:22:04.607622Z"}},"outputs":[{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ],\n       [ 2.75200369,  3.94947757,  2.75200369,  3.94947757,  5.20997639,\n        -5.05052243],\n       [ 7.93349184,  9.40367562,  7.93349184,  9.40367562, 10.9512375 ,\n         0.40367562],\n       ...,\n       [10.9512375 , 12.58025   , 10.9512375 ,  9.40367562,  1.9512375 ,\n         1.9512375 ],\n       [ 5.20997639,  6.53681725,  5.20997639,  6.53681725, -3.79002361,\n        -3.79002361],\n       [16.1       , 14.295     , 16.1       , 18.        ,  7.1       ,\n         7.1       ]])"},"metadata":{}}],"execution_count":159},{"cell_type":"code","source":"# Evaluate the trained agent\nmean_reward, std_reward = evaluate_agent(env, qTable_taxi, n_eval_episodes=100, max_steps=99) \nprint(f\"Mean Reward: {mean_reward} +/- {std_reward}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:22:15.983321Z","iopub.execute_input":"2025-06-19T08:22:15.984206Z","iopub.status.idle":"2025-06-19T08:22:16.039389Z","shell.execute_reply.started":"2025-06-19T08:22:15.984151Z","shell.execute_reply":"2025-06-19T08:22:16.038567Z"}},"outputs":[{"name":"stdout","text":"Mean Reward: 7.84 +/- 2.667283262047734\n","output_type":"stream"}],"execution_count":161}]}