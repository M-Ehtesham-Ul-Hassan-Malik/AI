{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh4AqGd2sGRY",
        "outputId": "7fd0c02e-783b-43c4-f646-caaa4068491d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-groq langchain langchain_core \"langchain-chroma>=0.1.2\" langchain_community sentence_transformers fastembed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO17SrGKsNx-",
        "outputId": "7314635c-72d0-410c-ca09-882e983ed909"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the ChatGroq class from the langchain_groq package.\n",
        "# This class helps us interact with the Groq-hosted LLMs (like LLaMA3) using LangChain.\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Creating a model instance of ChatGroq with some configuration.\n",
        "# This model will let us send text to LLaMA3 and get smart responses back.\n",
        "\n",
        "model = ChatGroq(\n",
        "    model=\"llama3-8b-8192\",  # This specifies which language model to use. Here, it's Meta's LLaMA3 (8 billion parameters).\n",
        "    temperature=0,           # Temperature controls creativity. 0 = more accurate & predictable, 1 = more creative & random.\n",
        "    max_tokens=None,         # This defines the max length of the response. 'None' means default value will be used.\n",
        "    timeout=None,            # No timeout set for the request (it will wait until it finishes).\n",
        "    max_retries=2            # If the model fails to respond (due to network or other issues), try again up to 2 times.\n",
        ")"
      ],
      "metadata": {
        "id": "nJFC4egMsc44"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the model to translate an English sentence into Urdu.\n",
        "# `.invoke()` sends the instruction to the LLM and gets the output.\n",
        "result = model.invoke(\"Translate this into Urdu: Pakistan won the 2025 cricket world cup against India\")\n",
        "\n",
        "# Printing the entire result object (which may include metadata + response).\n",
        "print(result)\n",
        "\n",
        "# Printing just the text content (actual response from the model).\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HKDLRGCtAUl",
        "outputId": "7da2b3c5-c100-4012-f259-5b4084d6f5e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='پاکستان نے 2025 کراچی ورلڈ کپ میں انڈیا کو شکست دی\\n\\n(Pakistan ne 2025 karachi world cup mein India ko shikast di)\\n\\nNote: Since the 2025 Cricket World Cup has not taken place yet, the translation is based on the assumption that Pakistan won the tournament.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 26, 'total_tokens': 103, 'completion_time': 0.116398131, 'prompt_time': 0.005314297, 'queue_time': 0.205769539, 'total_time': 0.121712428}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8dc6ecaf8e', 'finish_reason': 'stop', 'logprobs': None} id='run--76252ed3-8858-40da-8c84-3f1030fceb07-0' usage_metadata={'input_tokens': 26, 'output_tokens': 77, 'total_tokens': 103}\n",
            "پاکستان نے 2025 کراچی ورلڈ کپ میں انڈیا کو شکست دی\n",
            "\n",
            "(Pakistan ne 2025 karachi world cup mein India ko shikast di)\n",
            "\n",
            "Note: Since the 2025 Cricket World Cup has not taken place yet, the translation is based on the assumption that Pakistan won the tournament.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing message types from langchain_core:\n",
        "# These help define who is speaking (system, human, or AI).\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "AtyqOnwNtSpk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of messages to simulate a real chat conversation.\n",
        "# The first message is a SystemMessage (sets the behavior or goal for the AI),\n",
        "# The second is a HumanMessage (user's actual question or input).\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve this math problem\"),       # Instruction to the AI: \"You're here to solve math problems\"\n",
        "    HumanMessage(content=\"What is 2 + 65?\")                 # Actual question from the user\n",
        "]\n",
        "\n",
        "# Sending the list of messages to the model using `invoke`.\n",
        "# The model will look at the full chat history to generate a response.\n",
        "result = model.invoke(messages)\n",
        "\n",
        "# Printing the entire response object (which might include more than just the content).\n",
        "print(result)\n",
        "\n",
        "# Printing only the actual answer the AI gave, as plain text.\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v24CAR4zu61s",
        "outputId": "bb77c646-8f4f-43f3-d948-34d10a87292e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The answer to 2 + 65 is 67.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 28, 'total_tokens': 41, 'completion_time': 0.031470087, 'prompt_time': 0.023668934, 'queue_time': 0.27535075800000003, 'total_time': 0.055139021}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8dc6ecaf8e', 'finish_reason': 'stop', 'logprobs': None} id='run--5e3f6c54-8f6d-41f0-b468-270d513354a1-0' usage_metadata={'input_tokens': 28, 'output_tokens': 13, 'total_tokens': 41}\n",
            "The answer to 2 + 65 is 67.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import message types to simulate a chat: SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# Creating a simulated multi-turn chat history\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve this math porblem\"),         # System tells AI what role to play (math solver)\n",
        "\n",
        "    HumanMessage(content=\"What is 2 + 65?\"),                   # First question from the user\n",
        "\n",
        "    AIMessage(content=\"The answer is 67\"),                     # Simulated response from the AI (you pretend it already replied)\n",
        "\n",
        "    HumanMessage(content=\"What is 245 / 65?\")                  # New question from the user\n",
        "]\n",
        "\n",
        "# Sending the full conversation (history + new question) to the model\n",
        "result = model.invoke(messages)\n",
        "\n",
        "# Printing the full result (includes metadata, might show it's an AIMessage object)\n",
        "print(result)\n",
        "\n",
        "# Printing just the content of the AI's reply (the answer)\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmNKJzP-vR3m",
        "outputId": "4c1a96e5-e432-4175-ae01-ba4605f0ec02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The answer is 3.77' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 52, 'total_tokens': 60, 'completion_time': 0.009107211, 'prompt_time': 0.007133906, 'queue_time': 0.20690925100000002, 'total_time': 0.016241117}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8b7c3a83f7', 'finish_reason': 'stop', 'logprobs': None} id='run--fef95e67-0557-4f83-9d5d-07dcbe4693be-0' usage_metadata={'input_tokens': 52, 'output_tokens': 8, 'total_tokens': 60}\n",
            "The answer is 3.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing ChatPromptTemplate which helps create structured chat prompts\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "ZtOXceI6vYda"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the structure of the conversation using a list of (role, message) tuples\n",
        "# 'system' sets the personality/behavior of the AI\n",
        "# 'human' is the user input with variables {count} and {topic} to be filled dynamically\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes\"),\n",
        "    (\"human\", \"Tell me a {count} jokes about {topic}\"),\n",
        "]\n",
        "\n",
        "# Step 2: Create a ChatPromptTemplate from the defined messages\n",
        "# This lets you easily generate prompts by filling in the variables later\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "# Step 3: Fill the template with actual values for 'topic' and 'count'\n",
        "# In this case, you want 5 jokes about \"engineers\"\n",
        "prompt = prompt_template.invoke({\"topic\": \"engineers\", \"count\": 5})\n",
        "\n",
        "# Step 4: Print the full structured prompt (includes both system and human parts)\n",
        "print(prompt)\n",
        "\n",
        "# Step 5: Send the structured prompt to the language model (e.g., LLaMA3)\n",
        "# The model will read the prompt and generate a funny response (jokes)\n",
        "result = model.invoke(prompt)\n",
        "\n",
        "# Step 6: Print just the actual content (jokes) from the AI’s response\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H712fUWMvjaT",
        "outputId": "6b625262-49f0-40f8-a158-f7cc87dccba2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a comedian who tells jokes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a 5 jokes about engineers', additional_kwargs={}, response_metadata={})]\n",
            "Here are five jokes about engineers:\n",
            "\n",
            "1. Why did the engineer cross the road?\n",
            "\n",
            "To get to the other side... of the problem, because they're always trying to find a solution!\n",
            "\n",
            "2. Why did the engineer quit his job?\n",
            "\n",
            "Because he didn't get arrays! (get a raise)\n",
            "\n",
            "3. What did the engineer say when his wife asked him to take out the trash?\n",
            "\n",
            "\"I'm on it, I'll just design a more efficient garbage disposal system and then I'll get to it.\"\n",
            "\n",
            "4. Why did the engineer go to the doctor?\n",
            "\n",
            "Because he was feeling a little \"dis-connected\"!\n",
            "\n",
            "5. Why did the engineer go to the party?\n",
            "\n",
            "Because he heard it was a \"circuit\" party and he wanted to \"charge\" up his social life!\n",
            "\n",
            "Hope you found these jokes \"en-light-ening\"!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the output parser that extracts plain text from model responses\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Step 2: Define your chat prompt with a system role and a user question\n",
        "# This uses placeholders {count} and {topic} for dynamic values\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes\"),               # Tells the AI to act like a comedian\n",
        "    (\"human\", \"Tell me a {count} jokes about {topic}\"),             # Human asks the AI for jokes\n",
        "]\n",
        "\n",
        "\n",
        "# Step 3: Create a reusable ChatPromptTemplate from your message structure\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "\n",
        "# Step 4: Build a LangChain pipeline (a \"chain\") using the | (pipe) operator\n",
        "# The pipeline does the following:\n",
        "#    1. prompt_template: fill in the prompt with real values\n",
        "#    2. model: send the prompt to the LLM (e.g., llama3)\n",
        "#    3. StrOutputParser(): extract the raw string reply from the AI\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "\n",
        "response = StrOutputParser().invoke(\n",
        "    model.invoke(\n",
        "        prompt_template.invoke({\"topic\": \"engineers\", \"count\": 5})\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 5: Run the whole chain by giving it input values for the prompt\n",
        "# This will generate and print 5 jokes about engineers\n",
        "chain.invoke({\"topic\": \"engineers\", \"count\": 5})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "gClYg4T7wEDp",
        "outputId": "2b982dae-bd87-45b7-ada9-84eb7e15a891"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here are five jokes about engineers:\\n\\n1. Why did the engineer cross the road?\\n\\nTo get to the other side... of the problem, because they\\'re always trying to find a solution!\\n\\n2. Why did the engineer quit his job?\\n\\nBecause he didn\\'t get arrays! (get a raise)\\n\\n3. What did the engineer say when his wife asked him to take out the trash?\\n\\n\"I\\'m on it, I\\'ll just design a more efficient garbage disposal system and then I\\'ll get to it.\"\\n\\n4. Why did the engineer go to the doctor?\\n\\nBecause he was feeling a little \"dis-connected\"!\\n\\n5. Why did the engineer go to the party?\\n\\nBecause he heard it was a \"circuit\" party and he wanted to \"charge\" up his social life!\\n\\nHope you found these jokes \"en-light-ening\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing tools for creating conditional logic (branching) in a LangChain pipeline\n",
        "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
        "\n",
        "# Template for positive feedback\n",
        "# The AI will generate a polite thank-you message\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),  # Sets the AI's behavior\n",
        "        (\"human\", \"Generate a thank you note for this positive feedback: {feedback}.\"),  # User instruction\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Template for negative feedback\n",
        "# The AI will still thank the user but acknowledge the negative nature\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"Generate a thank you note for this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Template for neutral feedback\n",
        "# The AI will ask for more details to better understand the user's input\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"Generate a request for more details for this neutral feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Template for feedback that needs to be escalated to a human (e.g., serious complaints)\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"Generate a message to escalate this feedback to a human agent: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Before responding, we first ask the AI to classify the type of feedback:\n",
        "# Is it positive, negative, neutral, or should it be escalated?\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# RunnableBranch checks the classification result and chooses the correct response template accordingly\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,  # If feedback is classified as \"positive\"...\n",
        "        positive_feedback_template | model | StrOutputParser()  # ...use this prompt chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()\n",
        "    ),\n",
        "    # If none of the above conditions match, this is the default fallback (escalation)\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# This chain takes the original feedback and classifies its sentiment\n",
        "# Output will be one of: \"positive\", \"negative\", \"neutral\", or \"escalate\"\n",
        "classification_chain = (\n",
        "    classification_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# Final chain:\n",
        "# 1. Classify the feedback\n",
        "# 2. Based on result, pick the right response template\n",
        "chain = classification_chain | branches\n",
        "\n",
        "\n",
        "# Input: A positive review from a user\n",
        "review = \"The product is perfect. I loved it.\"\n",
        "\n",
        "# Running the full pipeline with the feedback\n",
        "# It will classify the sentiment and generate the appropriate reply\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Show the final AI-generated response\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKKEW1x7wZAd",
        "outputId": "57b045f8-c7a6-4ea2-a3d3-2c50f3ec822a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a potential thank-you note:\n",
            "\n",
            "Dear [Name],\n",
            "\n",
            "We're thrilled to hear that you've had a perfect experience with our product! Your enthusiasm and satisfaction mean the world to us, and we're so grateful that you've loved using it. Your positive feedback is a huge motivator for our team, and we're honored to have been able to bring a smile to your face.\n",
            "\n",
            "Thank you again for taking the time to share your thoughts with us. We're committed to continuing to deliver high-quality products that exceed your expectations, and we're excited to see what the future holds for our partnership.\n",
            "\n",
            "Best regards,\n",
            "[Your Name/Company]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  LangChain's ChatPromptTemplate to create a prompt that asks the AI to summarize a product, including its features, advantages, and disadvantages."
      ],
      "metadata": {
        "id": "gDJ1RalE2dXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a chat prompt template to analyze a product\n",
        "Product_detail = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # The system message sets the role or tone of the AI — in this case, as a helpful assistant\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "\n",
        "        # The human message is a question or instruction with a placeholder {product}\n",
        "        # When we run this template, we'll replace {product} with a real product name\n",
        "        (\"human\",\n",
        "         \"Generate the summary about the product, highlight the features, advantages and disadvantages of it: {product}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Preview the generated prompt by filling in the {product} placeholder\n",
        "# This does NOT send the prompt to the model — it just formats the messages\n",
        "Product_detail.invoke({\"product\": \"iPhone 15 Pro Max\"})\n",
        "\n",
        "\n",
        "# Step 3: Import a tool to extract plain text from the AI's response\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "\n",
        "# Step 4: Now build the full chain:\n",
        "# - First: Fill in the prompt\n",
        "# - Then: Send it to the model (e.g., LLaMA3)\n",
        "# - Finally: Extract just the string response using StrOutputParser\n",
        "chain = Product_detail | model | StrOutputParser()\n",
        "\n",
        "\n",
        "# Step 5: Run the chain by giving it a product name\n",
        "result = chain.invoke({\"product\": \"itel P70\"})\n",
        "\n",
        "\n",
        "# Step 6: Print the AI's product summary response\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH6JANPtz47J",
        "outputId": "009af2b3-3ee1-4987-db17-331a3023a6d0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a summary of the itel P70:\n",
            "\n",
            "**Summary:**\n",
            "The itel P70 is a budget-friendly smartphone designed for everyday use. It features a sleek design, impressive battery life, and a range of connectivity options. With its affordable price tag, the itel P70 is an excellent choice for those looking for a reliable and feature-packed device without breaking the bank.\n",
            "\n",
            "**Features:**\n",
            "\n",
            "* 5.5-inch HD+ display with 18:9 aspect ratio\n",
            "* Octa-core processor with 3GB RAM and 32GB internal storage (expandable up to 128GB)\n",
            "* 13MP rear camera with LED flash and 5MP front camera\n",
            "* 4G LTE connectivity with dual-SIM support\n",
            "* 4000mAh battery with fast charging support\n",
            "* Android 9.0 (Pie) operating system\n",
            "* Fingerprint sensor and face unlock for enhanced security\n",
            "* Dual-SIM support with dedicated microSD card slot\n",
            "\n",
            "**Advantages:**\n",
            "\n",
            "* Long-lasting battery life with fast charging support\n",
            "* Affordable price tag without compromising on features\n",
            "* Impressive display with 18:9 aspect ratio\n",
            "* Dual-SIM support with dedicated microSD card slot\n",
            "* Fingerprint sensor and face unlock for enhanced security\n",
            "* Android 9.0 (Pie) operating system for seamless performance\n",
            "\n",
            "**Disadvantages:**\n",
            "\n",
            "* Limited storage capacity (32GB) with no expandable storage option\n",
            "* No NFC support\n",
            "* No wireless charging support\n",
            "* Camera performance may not be as good as high-end devices\n",
            "* No IP68 water and dust resistance\n",
            "\n",
            "**Conclusion:**\n",
            "The itel P70 is an excellent choice for those looking for a budget-friendly smartphone with impressive battery life, a range of connectivity options, and a sleek design. While it may lack some advanced features, it offers great value for its price. If you're looking for a reliable and feature-packed device without breaking the bank, the itel P70 is definitely worth considering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKgMf8BH1KJI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}