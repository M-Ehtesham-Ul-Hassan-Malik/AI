{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12223659,"sourceType":"datasetVersion","datasetId":7701173}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:03.373371Z","iopub.execute_input":"2025-06-20T06:04:03.373657Z","iopub.status.idle":"2025-06-20T06:04:03.392944Z","shell.execute_reply.started":"2025-06-20T06:04:03.373639Z","shell.execute_reply":"2025-06-20T06:04:03.391884Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset/IMDB_Dataset.csv\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:03.394651Z","iopub.execute_input":"2025-06-20T06:04:03.394966Z","iopub.status.idle":"2025-06-20T06:04:03.399724Z","shell.execute_reply.started":"2025-06-20T06:04:03.394934Z","shell.execute_reply":"2025-06-20T06:04:03.398854Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset/IMDB_Dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:03.400628Z","iopub.execute_input":"2025-06-20T06:04:03.400934Z","iopub.status.idle":"2025-06-20T06:04:04.136478Z","shell.execute_reply.started":"2025-06-20T06:04:03.400910Z","shell.execute_reply":"2025-06-20T06:04:04.135520Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.137448Z","iopub.execute_input":"2025-06-20T06:04:04.137741Z","iopub.status.idle":"2025-06-20T06:04:04.146737Z","shell.execute_reply.started":"2025-06-20T06:04:04.137710Z","shell.execute_reply":"2025-06-20T06:04:04.145683Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  I grew up (b. 1965) watching and loving the Th...  negative\n1  When I put this movie in my DVD player, and sa...  negative\n2  Why do people who do not know what a particula...  negative\n3  Even though I have great interest in Biblical ...  negative\n4  Im a die hard Dads Army fan and nothing will e...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I grew up (b. 1965) watching and loving the Th...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>When I put this movie in my DVD player, and sa...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why do people who do not know what a particula...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Even though I have great interest in Biblical ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Im a die hard Dads Army fan and nothing will e...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"df['review'][1].lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.149368Z","iopub.execute_input":"2025-06-20T06:04:04.149603Z","iopub.status.idle":"2025-06-20T06:04:04.167910Z","shell.execute_reply.started":"2025-06-20T06:04:04.149585Z","shell.execute_reply":"2025-06-20T06:04:04.166971Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"\"when i put this movie in my dvd player, and sat down with a coke and some chips, i had some expectations. i was hoping that this movie would contain some of the strong-points of the first movie: awsome animation, good flowing story, excellent voice cast, funny comedy and a kick-ass soundtrack. but, to my disappointment, not any of this is to be found in atlantis: milo's return. had i read some reviews first, i might not have been so let down. the following paragraph will be directed to those who have seen the first movie, and who enjoyed it primarily for the points mentioned.<br /><br />when the first scene appears, your in for a shock if you just picked atlantis: milo's return from the display-case at your local videoshop (or whatever), and had the expectations i had. the music feels as a bad imitation of the first movie, and the voice cast has been replaced by a not so fitting one. (with the exception of a few characters, like the voice of sweet). the actual drawings isnt that bad, but the animation in particular is a sad sight. the storyline is also pretty weak, as its more like three episodes of schooby-doo than the single adventurous story we got the last time. but dont misunderstand, it's not very good schooby-doo episodes. i didnt laugh a single time, although i might have sniggered once or twice.<br /><br />to the audience who haven't seen the first movie, or don't especially care for a similar sequel, here is a fast review of this movie as a stand-alone product: if you liked schooby-doo, you might like this movie. if you didn't, you could still enjoy this movie if you have nothing else to do. and i suspect it might be a good kids movie, but i wouldn't know. it might have been better if milo's return had been a three-episode series on a cartoon channel, or on breakfast tv.\""},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"df['review'] = df['review'].str.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.168795Z","iopub.execute_input":"2025-06-20T06:04:04.169030Z","iopub.status.idle":"2025-06-20T06:04:04.388011Z","shell.execute_reply.started":"2025-06-20T06:04:04.169011Z","shell.execute_reply":"2025-06-20T06:04:04.386938Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.389026Z","iopub.execute_input":"2025-06-20T06:04:04.389364Z","iopub.status.idle":"2025-06-20T06:04:04.402442Z","shell.execute_reply.started":"2025-06-20T06:04:04.389339Z","shell.execute_reply":"2025-06-20T06:04:04.401431Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n0      i grew up (b. 1965) watching and loving the th...  negative\n1      when i put this movie in my dvd player, and sa...  negative\n2      why do people who do not know what a particula...  negative\n3      even though i have great interest in biblical ...  negative\n4      im a die hard dads army fan and nothing will e...  positive\n...                                                  ...       ...\n39995  \"western union\" is something of a forgotten cl...  positive\n39996  this movie is an incredible piece of work. it ...  positive\n39997  my wife and i watched this movie because we pl...  negative\n39998  when i first watched flatliners, i was amazed....  positive\n39999  why would this film be so good, but only gross...  positive\n\n[40000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i grew up (b. 1965) watching and loving the th...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>when i put this movie in my dvd player, and sa...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>why do people who do not know what a particula...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>even though i have great interest in biblical ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>im a die hard dads army fan and nothing will e...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>\"western union\" is something of a forgotten cl...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>this movie is an incredible piece of work. it ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>my wife and i watched this movie because we pl...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>when i first watched flatliners, i was amazed....</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>why would this film be so good, but only gross...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"import re # importing regular expression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.403399Z","iopub.execute_input":"2025-06-20T06:04:04.403707Z","iopub.status.idle":"2025-06-20T06:04:04.422078Z","shell.execute_reply.started":"2025-06-20T06:04:04.403673Z","shell.execute_reply":"2025-06-20T06:04:04.421147Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Function to remove HTML tags from a given text\ndef remove_html_tag(text):\n    \n    # Compile a regular expression pattern that matches anything between < and >\n    pattern = re.compile('<.*?>')\n    \n    # Replace all matches (HTML tags) in the text with an empty string\n    return pattern.sub(r'', text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.423065Z","iopub.execute_input":"2025-06-20T06:04:04.423807Z","iopub.status.idle":"2025-06-20T06:04:04.439804Z","shell.execute_reply.started":"2025-06-20T06:04:04.423779Z","shell.execute_reply":"2025-06-20T06:04:04.438878Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"remove_html_tag(\"<p>Hello <b>world</b></p>\")\n# Output: 'Hello world'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.441199Z","iopub.execute_input":"2025-06-20T06:04:04.441522Z","iopub.status.idle":"2025-06-20T06:04:04.461915Z","shell.execute_reply.started":"2025-06-20T06:04:04.441492Z","shell.execute_reply":"2025-06-20T06:04:04.461025Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'Hello world'"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"# Apply the remove_html_tag function to everay value in the 'review' column\n# This will clean each review by removing any HTML tags\ndf['review'] = df['review'].apply(remove_html_tag)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.462875Z","iopub.execute_input":"2025-06-20T06:04:04.463207Z","iopub.status.idle":"2025-06-20T06:04:04.679002Z","shell.execute_reply.started":"2025-06-20T06:04:04.463176Z","shell.execute_reply":"2025-06-20T06:04:04.677783Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Function to remove URLs from a given text\ndef remove_url(text):\n    # Regex pattern to match most common URL formats (http, https, www)\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    \n    # Replace the matched URLs with an empty string\n    return pattern.sub(r'', text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.680797Z","iopub.execute_input":"2025-06-20T06:04:04.681138Z","iopub.status.idle":"2025-06-20T06:04:04.687103Z","shell.execute_reply.started":"2025-06-20T06:04:04.681107Z","shell.execute_reply":"2025-06-20T06:04:04.686117Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"remove_url(\"Check this out: https://example.com and also visit www.test.com\")\n# Output: 'Check this out:  and also visit '","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.688025Z","iopub.execute_input":"2025-06-20T06:04:04.688394Z","iopub.status.idle":"2025-06-20T06:04:04.710496Z","shell.execute_reply.started":"2025-06-20T06:04:04.688342Z","shell.execute_reply":"2025-06-20T06:04:04.709534Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"'Check this out:  and also visit '"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"import string, time\nstring.punctuation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.715288Z","iopub.execute_input":"2025-06-20T06:04:04.715697Z","iopub.status.idle":"2025-06-20T06:04:04.730451Z","shell.execute_reply.started":"2025-06-20T06:04:04.715676Z","shell.execute_reply":"2025-06-20T06:04:04.729465Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"import string  # Import the string module to access built-in string constants\n\n# Define a string of all punctuation characters (e.g., !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\nexclude = string.punctuation\n\n# Function to remove all punctuation characters from a given text\ndef remove_punctuation(text):\n    # Loop through each punctuation character\n    for char in exclude:\n        # Replace the punctuation character with an empty string\n        text = text.replace(char, \"\")\n    # Return the cleaned text with punctuation removed\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.731734Z","iopub.execute_input":"2025-06-20T06:04:04.732072Z","iopub.status.idle":"2025-06-20T06:04:04.745076Z","shell.execute_reply.started":"2025-06-20T06:04:04.732046Z","shell.execute_reply":"2025-06-20T06:04:04.744220Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"remove_punctuation(\"Hello, world! How's it going?\")\n# Output: 'Hello world Hows it going'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.746098Z","iopub.execute_input":"2025-06-20T06:04:04.746435Z","iopub.status.idle":"2025-06-20T06:04:04.762471Z","shell.execute_reply.started":"2025-06-20T06:04:04.746408Z","shell.execute_reply":"2025-06-20T06:04:04.761434Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"'Hello world Hows it going'"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"def remove_punctuation1(text):\n    # Create a translation table that maps each punctuation character to None\n    # Then apply it to the text using translate()\n    return text.translate(str.maketrans('', '', exclude))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.763527Z","iopub.execute_input":"2025-06-20T06:04:04.763842Z","iopub.status.idle":"2025-06-20T06:04:04.780600Z","shell.execute_reply.started":"2025-06-20T06:04:04.763815Z","shell.execute_reply":"2025-06-20T06:04:04.779508Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"remove_punctuation1(\"Hello, world! How's it going?\")\n# Output: 'Hello world Hows it going'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.781661Z","iopub.execute_input":"2025-06-20T06:04:04.782131Z","iopub.status.idle":"2025-06-20T06:04:04.799434Z","shell.execute_reply.started":"2025-06-20T06:04:04.782098Z","shell.execute_reply":"2025-06-20T06:04:04.798523Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"'Hello world Hows it going'"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"# Sample text\nsample_text = \"Hello, world! How's it going? #Python @2025\"\n\n# Start the timer\nstart_time = time.time()\n\n# Calling the manual function \ncleaned_text = remove_punctuation(sample_text)\n\n# End the timer\nend_time = time.time()\n\n# Output the result and time taken\nprint(\"Cleaned Text:\", cleaned_text)\nprint(\"Time Taken:\", end_time - start_time, \"seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.800615Z","iopub.execute_input":"2025-06-20T06:04:04.800980Z","iopub.status.idle":"2025-06-20T06:04:04.816973Z","shell.execute_reply.started":"2025-06-20T06:04:04.800940Z","shell.execute_reply":"2025-06-20T06:04:04.815823Z"}},"outputs":[{"name":"stdout","text":"Cleaned Text: Hello world Hows it going Python 2025\nTime Taken: 0.0001316070556640625 seconds\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# Sample text\nsample_text = \"Hello, world! How's it going? #Python @2025\"\n\n# Start the timer\nstart_time = time.time()\n\n# Call the  builtin function\ncleaned_text = remove_punctuation1(sample_text)\n\n# End the timer\nend_time = time.time()\n\n# Output the result and time taken\nprint(\"Cleaned Text:\", cleaned_text)\nprint(\"Time Taken:\", end_time - start_time, \"seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.817925Z","iopub.execute_input":"2025-06-20T06:04:04.819171Z","iopub.status.idle":"2025-06-20T06:04:04.845592Z","shell.execute_reply.started":"2025-06-20T06:04:04.819123Z","shell.execute_reply":"2025-06-20T06:04:04.844589Z"}},"outputs":[{"name":"stdout","text":"Cleaned Text: Hello world Hows it going Python 2025\nTime Taken: 7.2479248046875e-05 seconds\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"# What is TextBlob?\n**TextBlob** is a simple **NLP** (Natural Language Processing) tool that provides:\n\n* Spelling Correction\n\n* Sentiment Analysis\n\n* Translation\n\n* Part-of-Speech Tagging\n\n* Tokenization (splitting into words/sentences)","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.846575Z","iopub.execute_input":"2025-06-20T06:04:04.846882Z","iopub.status.idle":"2025-06-20T06:04:04.862455Z","shell.execute_reply.started":"2025-06-20T06:04:04.846859Z","shell.execute_reply":"2025-06-20T06:04:04.861395Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"text = \"TextBlob is amazingly simple to use. What a great module!\"\nblob = TextBlob(text)\n\n# Sentiment analysis\nprint(blob.sentiment)  # Sentiment(polarity=0.85, subjectivity=0.75)\n\n# Correct spelling\nprint(TextBlob(\"I havv goood speling\").correct())  # I have good spelling\n\n# Word tokenization\nprint(blob.words)  # ['TextBlob', 'is', 'amazingly', 'simple', 'to', 'use', 'What', 'a', 'great', 'module']\n\n# Language translation\n# print(blob.translate(to='fr'))  # Uncomment if you want to try translating (requires internet)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.863606Z","iopub.execute_input":"2025-06-20T06:04:04.863929Z","iopub.status.idle":"2025-06-20T06:04:04.884314Z","shell.execute_reply.started":"2025-06-20T06:04:04.863901Z","shell.execute_reply":"2025-06-20T06:04:04.883208Z"}},"outputs":[{"name":"stdout","text":"Sentiment(polarity=0.5, subjectivity=0.5535714285714286)\nI have good spelling\n['TextBlob', 'is', 'amazingly', 'simple', 'to', 'use', 'What', 'a', 'great', 'module']\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"#  What is NLTK?\nNLTK provides tools for:\n\n* Tokenization (splitting text into words/sentences)\n\n* Stopword removal\n\n* Part-of-Speech tagging\n\n* Named Entity Recognition (NER)\n\n* Stemming & Lemmatization\n\n* Text classification\n\n* And much more","metadata":{}},{"cell_type":"code","source":"import nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.885005Z","iopub.execute_input":"2025-06-20T06:04:04.885360Z","iopub.status.idle":"2025-06-20T06:04:04.902093Z","shell.execute_reply.started":"2025-06-20T06:04:04.885335Z","shell.execute_reply":"2025-06-20T06:04:04.901126Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.903137Z","iopub.execute_input":"2025-06-20T06:04:04.903519Z","iopub.status.idle":"2025-06-20T06:04:04.916656Z","shell.execute_reply.started":"2025-06-20T06:04:04.903498Z","shell.execute_reply":"2025-06-20T06:04:04.915745Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# nltk.download('stopwords')\n# nltk.download('punkt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.917625Z","iopub.execute_input":"2025-06-20T06:04:04.917916Z","iopub.status.idle":"2025-06-20T06:04:04.932773Z","shell.execute_reply.started":"2025-06-20T06:04:04.917892Z","shell.execute_reply":"2025-06-20T06:04:04.931734Z"}},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":"## Remove Stop Words","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text):\n    # Tokenize the input text into words\n    words = word_tokenize(text)\n    \n    # Get the list of English stopwords\n    stop_words = set(stopwords.words('english'))\n    \n    # Filter out stopwords\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    \n    # Return the filtered text as a string\n    return ' '.join(filtered_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.933719Z","iopub.execute_input":"2025-06-20T06:04:04.934034Z","iopub.status.idle":"2025-06-20T06:04:04.948330Z","shell.execute_reply.started":"2025-06-20T06:04:04.934010Z","shell.execute_reply":"2025-06-20T06:04:04.947363Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"sample_text = \"This is a sample sentence, showing off the stop words filtration.\"\nclean_text = remove_stopwords(sample_text)\n\nprint(\"Original:\", sample_text)\nprint(\"Without Stopwords:\", clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:04.949450Z","iopub.execute_input":"2025-06-20T06:04:04.949741Z","iopub.status.idle":"2025-06-20T06:04:04.966246Z","shell.execute_reply.started":"2025-06-20T06:04:04.949710Z","shell.execute_reply":"2025-06-20T06:04:04.965173Z"}},"outputs":[{"name":"stdout","text":"Original: This is a sample sentence, showing off the stop words filtration.\nWithout Stopwords: sample sentence , showing stop words filtration .\n","output_type":"stream"}],"execution_count":73},{"cell_type":"markdown","source":"## Emoji Removal","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_emojis(text):\n    # Emoji pattern that matches a wide range of emoji characters\n    emoji_pattern = re.compile(\n        \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons (e.g. 😀😄😉)\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs (e.g. 🌟🔥🚀)\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols (e.g. 🚗✈️🚤)\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (e.g. 🇺🇸🇮🇳)\n        u\"\\U00002700-\\U000027BF\"  # other symbols (e.g. ✂️✈️)\n        u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols and pictographs\n        u\"\\U00002600-\\U000026FF\"  # miscellaneous symbols (e.g. ☀️☔)\n        u\"\\U0001FA70-\\U0001FAFF\"  # extended symbols\n        \"]+\",\n        flags=re.UNICODE\n    )\n    return emoji_pattern.sub(r'', text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:06.929285Z","iopub.execute_input":"2025-06-20T06:04:06.929677Z","iopub.status.idle":"2025-06-20T06:04:06.935973Z","shell.execute_reply.started":"2025-06-20T06:04:06.929641Z","shell.execute_reply":"2025-06-20T06:04:06.934979Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"text = \"Good morning! ☀️😊 Ready to work? 💻🚀\"\nprint(remove_emojis(text))\n# Output: \"Good morning!  Ready to work? \"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:06.937959Z","iopub.execute_input":"2025-06-20T06:04:06.938355Z","iopub.status.idle":"2025-06-20T06:04:06.953099Z","shell.execute_reply.started":"2025-06-20T06:04:06.938309Z","shell.execute_reply":"2025-06-20T06:04:06.952096Z"}},"outputs":[{"name":"stdout","text":"Good morning! ️ Ready to work? \n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"## Function to Convert Emojis to Text","metadata":{}},{"cell_type":"code","source":"# pip install emoji","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:06.954293Z","iopub.execute_input":"2025-06-20T06:04:06.954555Z","iopub.status.idle":"2025-06-20T06:04:06.969422Z","shell.execute_reply.started":"2025-06-20T06:04:06.954536Z","shell.execute_reply":"2025-06-20T06:04:06.968321Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"import emoji\n\n# Function to replace emojis with their text names (like :smile:)\ndef demojize_text(text):\n    return emoji.demojize(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:06.970556Z","iopub.execute_input":"2025-06-20T06:04:06.970799Z","iopub.status.idle":"2025-06-20T06:04:06.985700Z","shell.execute_reply.started":"2025-06-20T06:04:06.970782Z","shell.execute_reply":"2025-06-20T06:04:06.984811Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"text = \"I love Python! 🐍🔥😊\"\nprint(demojize_text(text))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:06.987921Z","iopub.execute_input":"2025-06-20T06:04:06.988563Z","iopub.status.idle":"2025-06-20T06:04:07.002774Z","shell.execute_reply.started":"2025-06-20T06:04:06.988538Z","shell.execute_reply":"2025-06-20T06:04:07.001774Z"}},"outputs":[{"name":"stdout","text":"I love Python! :snake::fire::smiling_face_with_smiling_eyes:\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"# What is Tokenization in NLP?\nTokenization is the process of splitting text into smaller units (called tokens) — typically words or sentences.\nIn Python, the most common tools for tokenization are:\nNLTK\n\nspaCy\n\nTextBlob\n\nHugging Face Tokenizers (for deep learning)\n\nPython's .split() (very basic)","metadata":{}},{"cell_type":"markdown","source":"## Using NLTK (Recommended for learning and preprocessing)\n","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n# Download tokenizer models\n# nltk.download('punkt') \n\ntext = \"Hello there! How are you doing today?\"\n\n# Sentence Tokenization\nsentences = sent_tokenize(text)\nprint(\"Sentences:\", sentences)\n\n# Word Tokenization\nwords = word_tokenize(text)\nprint(\"Words:\", words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:53.071019Z","iopub.execute_input":"2025-06-20T06:04:53.071865Z","iopub.status.idle":"2025-06-20T06:04:53.077593Z","shell.execute_reply.started":"2025-06-20T06:04:53.071836Z","shell.execute_reply":"2025-06-20T06:04:53.076588Z"}},"outputs":[{"name":"stdout","text":"Sentences: ['Hello there!', 'How are you doing today?']\nWords: ['Hello', 'there', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"text = \"A.I is continuously evolving in U.S.A, U.A.E and in Pakistan.\"\n# Sentence Tokenization\nsentences = sent_tokenize(text)\nprint(\"Sentences:\", sentences)\n\n# Word Tokenization\nwords = word_tokenize(text)\nprint(\"Words:\", words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:10:22.833062Z","iopub.execute_input":"2025-06-20T06:10:22.833418Z","iopub.status.idle":"2025-06-20T06:10:22.839139Z","shell.execute_reply.started":"2025-06-20T06:10:22.833395Z","shell.execute_reply":"2025-06-20T06:10:22.838209Z"}},"outputs":[{"name":"stdout","text":"Sentences: ['A.I is continuously evolving in U.S.A, U.A.E and in Pakistan.']\nWords: ['A.I', 'is', 'continuously', 'evolving', 'in', 'U.S.A', ',', 'U.A.E', 'and', 'in', 'Pakistan', '.']\n","output_type":"stream"}],"execution_count":89},{"cell_type":"markdown","source":" ## Using TextBlob (Easy wrapper on top of NLTK)","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\ntext = TextBlob(\"TextBlob makes tokenization super easy. It's great!\")\n\nprint(\"Words:\", text.words)\nprint(\"Sentences:\", text.sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:04:07.022646Z","iopub.execute_input":"2025-06-20T06:04:07.022998Z","iopub.status.idle":"2025-06-20T06:04:07.040751Z","shell.execute_reply.started":"2025-06-20T06:04:07.022978Z","shell.execute_reply":"2025-06-20T06:04:07.039875Z"}},"outputs":[{"name":"stdout","text":"Words: ['TextBlob', 'makes', 'tokenization', 'super', 'easy', 'It', \"'s\", 'great']\nSentences: [Sentence(\"TextBlob makes tokenization super easy.\"), Sentence(\"It's great!\")]\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"## Using Python’s Built-in split() (Not very accurate)","metadata":{}},{"cell_type":"code","source":"text = \"This is a basic tokenizer example. I love to be simple.\"\nprint(text.split())\n# Output: ['This', 'is', 'a', 'basic', 'tokenizer', 'example.']\nprint(text.split('.'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:05:50.164589Z","iopub.execute_input":"2025-06-20T06:05:50.164911Z","iopub.status.idle":"2025-06-20T06:05:50.170231Z","shell.execute_reply.started":"2025-06-20T06:05:50.164888Z","shell.execute_reply":"2025-06-20T06:05:50.169206Z"}},"outputs":[{"name":"stdout","text":"['This', 'is', 'a', 'basic', 'tokenizer', 'example.', 'I', 'love', 'to', 'be', 'simple.']\n['This is a basic tokenizer example', ' I love to be simple', '']\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"text = \"A.I is continuously evolving in U.S.A, U.A.E and in Pakistan.\"\nprint(text.split())\nprint(text.split('.')) # wouldn't handle properly \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:07:21.573915Z","iopub.execute_input":"2025-06-20T06:07:21.574290Z","iopub.status.idle":"2025-06-20T06:07:21.579505Z","shell.execute_reply.started":"2025-06-20T06:07:21.574265Z","shell.execute_reply":"2025-06-20T06:07:21.578376Z"}},"outputs":[{"name":"stdout","text":"['A.I', 'is', 'continuously', 'evolving', 'in', 'U.S.A,', 'U.A.E', 'and', 'in', 'Pakistan.']\n['A', 'I is continuously evolving in U', 'S', 'A, U', 'A', 'E and in Pakistan', '']\n","output_type":"stream"}],"execution_count":87},{"cell_type":"markdown","source":"# Stemming vs Lemmatization\n| Feature               | **Stemming**                              | **Lemmatization**                        |\n| --------------------- | ----------------------------------------- | ---------------------------------------- |\n| 🔧 Method             | Rule-based (removes suffixes/prefixes)    | Dictionary-based (returns actual words)  |\n| 🧠 Linguistic Meaning | May not return real words                 | Always returns valid words               |\n| 🏃 Speed              | Fast (just cuts endings)                  | Slower (uses a dictionary & POS tagging) |\n| 🎯 Accuracy           | Lower — may cut too much or too little    | Higher — more meaningful base forms      |\n| 📚 Library (Python)   | `PorterStemmer`, `SnowballStemmer` (NLTK) | `WordNetLemmatizer` (NLTK)               |\n\n\n### Use Cases\n**Stemming is useful when**:\n\n- You need speed.\n- Precision isn’t a major concern (e.g., search engines).\n\n**Lemmatization is better when**:\n\n- You care about meaning and correct words.\n- You're doing tasks like sentiment analysis, text classification, etc.","metadata":{}},{"cell_type":"code","source":"# Import stemming and lemmatization tools from NLTK\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport nltk\n\n# These downloads are required only once to enable lemmatization\n# nltk.download('wordnet')       # WordNet is a lexical database used for lemmatization\n# nltk.download('omw-1.4')       # Open Multilingual WordNet (required for some words)\n\n# Create an instance of PorterStemmer (a popular stemming algorithm)\nstemmer = PorterStemmer()\n\n# Create an instance of WordNetLemmatizer (uses vocabulary & POS to find base word)\nlemmatizer = WordNetLemmatizer()\n\n# Sample word to process\nword = \"running\"\n\n# Apply stemming: chops off the suffix to get the root form\nprint(\"Stemmed:\", stemmer.stem(word))          \n# Output: run (but sometimes it might give non-dictionary words like 'studie' from 'studies')\n\n# Apply lemmatization: looks up dictionary base form (needs part of speech for accuracy)\nprint(\"Lemmatized:\", lemmatizer.lemmatize(word, pos=\"v\"))  \n# Output: run (POS tag \"v\" = verb; without it, it may not work properly)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:29:17.238119Z","iopub.execute_input":"2025-06-20T06:29:17.238969Z","iopub.status.idle":"2025-06-20T06:29:17.245053Z","shell.execute_reply.started":"2025-06-20T06:29:17.238942Z","shell.execute_reply":"2025-06-20T06:29:17.244113Z"}},"outputs":[{"name":"stdout","text":"Stemmed: run\nLemmatized: run\n","output_type":"stream"}],"execution_count":93},{"cell_type":"markdown","source":"### Common POS tags used in lemmatize(word, pos=...):\n| POS Code | Part of Speech | Example Word | Lemmatized Form |\n| -------- | -------------- | ------------ | --------------- |\n| `'n'`    | noun           | cars         | car             |\n| `'v'`    | verb           | running      | run             |\n| `'a'`    | adjective      | better       | good            |\n| `'r'`    | adverb         | faster       | fast            |\n","metadata":{}},{"cell_type":"code","source":"# But now try \"better\"\nprint(\"Stemmed:\", stemmer.stem(\"better\"))      # Output: better\nprint(\"Lemmatized:\", lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Output: good\n\n# Stemming doesn’t recognize that “better” is a form of “good”, but lemmatization does.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T06:26:36.126973Z","iopub.execute_input":"2025-06-20T06:26:36.127295Z","iopub.status.idle":"2025-06-20T06:26:36.132870Z","shell.execute_reply.started":"2025-06-20T06:26:36.127272Z","shell.execute_reply":"2025-06-20T06:26:36.131921Z"}},"outputs":[{"name":"stdout","text":"Stemmed: better\nLemmatized: good\n","output_type":"stream"}],"execution_count":92},{"cell_type":"markdown","source":"# What are **Dense Embeddings** in NLP?\n\n**Dense embeddings** are **vector representations** of words, phrases, or sentences where:\n\n* Each word is represented by a **low-dimensional**, **dense** vector (e.g., 50–768 dimensions).\n* All entries in the vector usually have **real (float) values**.\n* These vectors are **learned from data** and capture **semantic meaning** — similar words are close in vector space.\n\n---\n\n###  Why \"Dense\"?\n\n* **Dense** means the vector is **compact** and **mostly non-zero**.\n* Contrast with **sparse vectors**, like one-hot encodings, which are mostly zeros.\n\n---\n\n###  Example:\n\nSuppose the word `\"king\"` is represented like this:\n\n```python\nking → [0.21, -0.17, 0.04, ..., 0.88]  # 100-dimensional vector\n```\n\nEach number is a **feature** learned from data — capturing things like gender, royalty, relationships, etc.\n\n---\n\n###  Dense Embedding Techniques\n\n| Technique               | Description                                    | Output                     |\n| ----------------------- | ---------------------------------------------- | -------------------------- |\n| **Word2Vec**            | Learns word embeddings based on context window | Word vectors               |\n| **GloVe**               | Learns embeddings based on word co-occurrence  | Word vectors               |\n| **FastText**            | Like Word2Vec but uses subword information     | Word vectors               |\n| **BERT / Transformers** | Contextual embeddings for sentences & words    | Word + sentence embeddings |\n\n---\n\n###  Dense vs Sparse Embeddings\n\n| Feature           | Sparse (e.g., One-hot)       | Dense (e.g., Word2Vec, BERT) |\n| ----------------- | ---------------------------- | ---------------------------- |\n| Dimensions        | Very high (e.g., vocab size) | Low (e.g., 50, 300, 768)     |\n| Mostly Zeros?     | Yes                          | No                           |\n| Learns Semantics? | ❌                            | ✅                            |\n| Memory Efficient? | ❌                            | ✅                            |\n\n---\n\n###  Example with Word2Vec (conceptual):\n\n```python\nsimilarity(\"king\", \"queen\") → high\nsimilarity(\"king\", \"apple\") → low\n```\n\nDense embeddings capture this kind of **semantic similarity**.\n\n---\n\n###  Summary:\n\n* Dense embeddings turn text into **meaningful numerical form**.\n* They're essential for machine learning and deep learning models.\n* They're **learned**, not manually designed, and encode **semantic relationships**.\n\nLet me know if you want to generate embeddings using Word2Vec, GloVe, or BERT!\n","metadata":{}}]}