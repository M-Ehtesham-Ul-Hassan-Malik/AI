# Linear Regression
It is a type of **Supervised Machine Learning Algorithm** that learns from the labeled dataset. It is the fundamental statical method used to model the relationship between one or more independent variables(features) and a continuous dependent variable(target variable), assuming that the **relationship is linear**.
### Goal of Linear Regression
To **find the best fitting line** by minimizing distance between the actual datapoints and predicted values.

---

###  Key Assumptions of Linear Regression
1. **Linearity**
The relationship between independent variables and the dependent variable is linear.
2. **Independence**
Observations (and their residuals) are independent of each other.
3. **Homoscedasticity**
The residuals (errors) have constant variance at every level of the independent variable(s).
4. **Normality of Errors**	
The residuals are normally distributed (important for confidence intervals and hypothesis tests).
5. **No Multicollinearity** (for multiple linear regression)
Independent variables are not highly correlated with each other.


---

## Types of Linear Regression
Here are the main types based on the number of predictors(features) and other characteristics:
1. Simple Linear Regression
2. Multiple Linear Regression
3. Polynomial Regression
4. Ridge Regression (L2 Regularization)
5. Lasso Regression (L1 Regularization)
6. Elastic Net Regression
---

### Simple Linear Regression
It is useful when we want to predict the target variable using a single feature. It assumes the the straight line relationship between them.

#### Example
Suppose you are analyzing the how a student's hours of study(x) affect his test/exam score(y).

#### Formula
The equation for simple linear regression is:
$$
y = mx+b
$$
Where:

ğ‘¦ = predicted value (dependent variable)

ğ‘¥ = input value (independent variable)

ğ‘š = slope of the line (how much ğ‘¦ changes for a unit change in ğ‘¥)

b = y-intercept (value of ğ‘¦ when ğ‘¥ = 0)

---


### Multiple Linear Regression
Multiple Linear Regression is the extension of Simple Linear Regression. Instead of using one independent variable it uses two or more independent variable. 
#### Example
Pridicting the price of the car on the bases of multiple features or input variables.
**Features**: Brand, Year, Engine Capacity, Mileage.
**Traget Variable**: Price of the car.

#### Formula
$$
y = ğ‘¤_1 ğ‘¥_1 + â€¦ + ğ‘¤_ğ‘š ğ‘¥_ğ‘š + b
$$

#### Use Case of Multiple Linear Regression
1. **Forecasting**: e.g., sales, prices, growth

2. **Agricultural Yield Prediction**: Farmers can use MLR to estimate crop yields based on several variables like rainfall, temperature, soil quality and fertilizer usage. This information helps in planning agricultural practices for optimal productivity

3. **E-commerce Sales Analysis**: An e-commerce company can utilize MLR to assess how various factors such as product price, marketing promotions and seasonal trends impact sales.

---

### Polynomial Linear Regression (Technically non-linear, but based on linear regression)
Polynomial Linear Regression is a type of regression analysis where the relationship between the independent variable ğ‘¥ and the dependent variable ğ‘¦ is modeled as an ğ‘›-th degree polynomial in ğ‘¥ and the correspondingÂ conditional meanÂ ofÂ ğ‘¦.
#### Formula 
$$
ğ‘¦ = ğ‘¤_1 ğ‘¥ + ğ‘¤_2 ğ‘¥^(2 )+ğ‘¤_3 ğ‘¥^(3)+ â€¦ + ğ‘¤_ğ‘‘ ğ‘¥^ğ‘‘  + b
$$

#### Non-linearity in terms of ğ‘¥:
The relationship between ğ‘¦ and ğ‘¥ is non-linear because ğ‘¦ depends on powers of ğ‘¥(like ğ‘¥^(2 ), ğ‘¥^(3 ), etc). This means the curve fitting ğ‘¦ is a polynomial curve, not a straight line.

#### Why use Polynomial Linear Regression?
- To capture curvature in the data that simple linear regression can't.
- Easy to implement and compute because it's still linear in parameters.
- Flexibility by choosing the degree ğ‘› of the polynomial.

---


### Ridge Regression (L2 Regularization)
- Adds a penalty term proportional to the square of the magnitude of coefficients.
- Helps reduce overfitting by shrinking coefficients toward zero but does not set them exactly to zero.
- Useful when many features are correlated or when you want to keep all features but with smaller weights. 
---

### Lasso Regression (L1 Regularization)
- Adds a penalty term proportional to the absolute value of coefficients.
- Encourages sparsity by driving some coefficients exactly to zero, performing feature selection automatically.
- Useful when you want a simpler model with fewer features.
---

### Elastic Net Regression
- Combines both L1 and L2 penalties.
- Balances shrinkage and feature selection.
- Effective when there are multiple correlated features, encouraging group selection and sparsity.
